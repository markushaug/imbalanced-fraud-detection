{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MO-GAAL Code\n",
    "\"\"\"Multiple-Objective Generative Adversarial Active Learning.\n",
    "Part of the codes are adapted from\n",
    "https://github.com/leibinghe/GAAL-based-outlier-detection\n",
    "\"\"\"\n",
    "# Author: Winston Li <jk_zhengli@hotmail.com>\n",
    "# License: BSD 2 clause\n",
    "\n",
    "# Code slightly updated to run on Keras 3 by Markus Haug\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from pyod.models.base import BaseDetector\n",
    "from pyod.models.gaal_base import create_discriminator, create_generator\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "class MO_GAAL(BaseDetector):\n",
    "    \"\"\"Multi-Objective Generative Adversarial Active Learning.\n",
    "\n",
    "    MO_GAAL directly generates informative potential outliers to assist the\n",
    "    classifier in describing a boundary that can separate outliers from normal\n",
    "    data effectively. Moreover, to prevent the generator from falling into the\n",
    "    mode collapsing problem, the network structure of SO-GAAL is expanded from\n",
    "    a single generator (SO-GAAL) to multiple generators with different\n",
    "    objectives (MO-GAAL) to generate a reasonable reference distribution for\n",
    "    the whole dataset.\n",
    "    Read more in the :cite:`liu2019generative`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        The amount of contamination of the data set, i.e.\n",
    "        the proportion of outliers in the data set. Used when fitting to\n",
    "        define the threshold on the decision function.\n",
    "\n",
    "    k : int, optional (default=10)\n",
    "        The number of sub generators.\n",
    "\n",
    "    stop_epochs : int, optional (default=20)\n",
    "        The number of epochs of training. The number of total epochs equals to three times of stop_epochs.\n",
    "\n",
    "    lr_d : float, optional (default=0.01)\n",
    "        The learn rate of the discriminator.\n",
    "\n",
    "    lr_g : float, optional (default=0.0001)\n",
    "        The learn rate of the generator.\n",
    "\n",
    "\n",
    "    momentum : float, optional (default=0.9)\n",
    "        The momentum parameter for SGD.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    decision_scores_ : numpy array of shape (n_samples,)\n",
    "        The outlier scores of the training data.\n",
    "        The higher, the more abnormal. Outliers tend to have higher\n",
    "        scores. This value is available once the detector is fitted.\n",
    "\n",
    "    threshold_ : float\n",
    "        The threshold is based on ``contamination``. It is the\n",
    "        ``n_samples * contamination`` most abnormal samples in\n",
    "        ``decision_scores_``. The threshold is calculated for generating\n",
    "        binary outlier labels.\n",
    "\n",
    "    labels_ : int, either 0 or 1\n",
    "        The binary labels of the training data. 0 stands for inliers\n",
    "        and 1 for outliers/anomalies. It is generated by applying\n",
    "        ``threshold_`` on ``decision_scores_``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=10, stop_epochs=20, lr_d=0.01, lr_g=0.0001, momentum=0.9, contamination=0.1):\n",
    "        super(MO_GAAL, self).__init__(contamination=contamination)\n",
    "        self.k = k\n",
    "        self.stop_epochs = stop_epochs\n",
    "        self.lr_d = lr_d\n",
    "        self.lr_g = lr_g\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit detector. y is ignored in unsupervised methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "\n",
    "        y : Ignored\n",
    "            Not used, present for API consistency by convention.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "\n",
    "        X = check_array(X)\n",
    "        self._set_n_classes(y)\n",
    "        self.train_history = defaultdict(list)\n",
    "        names = locals()\n",
    "        epochs = self.stop_epochs * 3\n",
    "        stop = 0\n",
    "        latent_size = X.shape[1]\n",
    "        data_size = X.shape[0]\n",
    "        # Create discriminator\n",
    "        self.discriminator = create_discriminator(latent_size, data_size)\n",
    "        self.discriminator.compile(\n",
    "            optimizer=SGD(learning_rate=self.lr_d, momentum=self.momentum), loss='binary_crossentropy')\n",
    "\n",
    "        # Create k combine models\n",
    "        for i in range(self.k):\n",
    "            names['sub_generator' + str(i)] = create_generator(latent_size)\n",
    "            latent = Input(shape=(latent_size,))\n",
    "            names['fake' + str(i)] = names['sub_generator' + str(i)](latent)\n",
    "            self.discriminator.trainable = False\n",
    "            names['fake' + str(i)] = self.discriminator(names['fake' + str(i)])\n",
    "            names['combine_model' + str(i)] = Model(latent,\n",
    "                                                    names['fake' + str(i)])\n",
    "            names['combine_model' + str(i)].compile(\n",
    "                optimizer=SGD(learning_rate=self.lr_g,\n",
    "                              momentum=self.momentum),\n",
    "                loss='binary_crossentropy')\n",
    "\n",
    "        # Start iteration\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch {} of {}'.format(epoch + 1, epochs))\n",
    "            batch_size = min(500, data_size)\n",
    "            num_batches = int(data_size / batch_size)\n",
    "\n",
    "            for index in range(num_batches):\n",
    "                print('\\nTesting for epoch {} index {}:'.format(epoch + 1,\n",
    "                                                                index + 1))\n",
    "\n",
    "                # Generate noise\n",
    "                noise_size = batch_size\n",
    "                noise = np.random.uniform(0, 1, (int(noise_size), latent_size))\n",
    "\n",
    "                # Get training data\n",
    "                data_batch = X[index * batch_size: (index + 1) * batch_size]\n",
    "\n",
    "                # Generate potential outliers\n",
    "                block = ((1 + self.k) * self.k) // 2\n",
    "                for i in range(self.k):\n",
    "                    if i != (self.k - 1):\n",
    "                        noise_start = int(\n",
    "                            (((self.k + (self.k - i + 1)) * i) / 2) * (\n",
    "                                    noise_size // block))\n",
    "                        noise_end = int(\n",
    "                            (((self.k + (self.k - i)) * (i + 1)) / 2) * (\n",
    "                                    noise_size // block))\n",
    "                        names['noise' + str(i)] = noise[noise_start:noise_end]\n",
    "                        names['generated_data' + str(i)] = names[\n",
    "                            'sub_generator' + str(i)].predict(\n",
    "                            names['noise' + str(i)], verbose=0)\n",
    "                    else:\n",
    "                        noise_start = int(\n",
    "                            (((self.k + (self.k - i + 1)) * i) / 2) * (\n",
    "                                    noise_size // block))\n",
    "                        names['noise' + str(i)] = noise[noise_start:noise_size]\n",
    "                        names['generated_data' + str(i)] = names[\n",
    "                            'sub_generator' + str(i)].predict(\n",
    "                            names['noise' + str(i)], verbose=0)\n",
    "\n",
    "                # Concatenate real data to generated data\n",
    "                for i in range(self.k):\n",
    "                    if i == 0:\n",
    "                        x = np.concatenate(\n",
    "                            (data_batch, names['generated_data' + str(i)]))\n",
    "                    else:\n",
    "                        x = np.concatenate(\n",
    "                            (x, names['generated_data' + str(i)]))\n",
    "                y = np.array([1] * batch_size + [0] * int(noise_size))\n",
    "\n",
    "                # Train discriminator\n",
    "                discriminator_loss = self.discriminator.train_on_batch(x, y)\n",
    "                self.train_history['discriminator_loss'].append(\n",
    "                    discriminator_loss)\n",
    "\n",
    "                # Get the target value of sub-generator\n",
    "                pred_scores = self.discriminator.predict(X).ravel()\n",
    "\n",
    "                for i in range(self.k):\n",
    "                    names['T' + str(i)] = np.percentile(pred_scores,\n",
    "                                                        i / self.k * 100)\n",
    "                    names['trick' + str(i)] = np.array(\n",
    "                        [float(names['T' + str(i)])] * noise_size)\n",
    "\n",
    "                # Train generator\n",
    "                noise = np.random.uniform(0, 1, (int(noise_size), latent_size))\n",
    "                if stop == 0:\n",
    "                    for i in range(self.k):\n",
    "                        names['sub_generator' + str(i) + '_loss'] = \\\n",
    "                            names['combine_model' + str(i)].train_on_batch(\n",
    "                                noise, names['trick' + str(i)])\n",
    "                        self.train_history[\n",
    "                            'sub_generator{}_loss'.format(i)].append(\n",
    "                            names['sub_generator' + str(i) + '_loss'])\n",
    "                else:\n",
    "                    for i in range(self.k):\n",
    "                        names['sub_generator' + str(i) + '_loss'] = names[\n",
    "                            'combine_model' + str(i)].evaluate(noise, names[\n",
    "                            'trick' + str(i)])\n",
    "                        self.train_history[\n",
    "                            'sub_generator{}_loss'.format(i)].append(\n",
    "                            names['sub_generator' + str(i) + '_loss'])\n",
    "\n",
    "                generator_loss = 0\n",
    "                for i in range(self.k):\n",
    "                    # Access the last loss value which is the most recent one\n",
    "                    generator_loss += names['sub_generator' + str(i) + '_loss'][-1]\n",
    "\n",
    "                generator_loss = generator_loss / self.k\n",
    "                self.train_history['generator_loss'].append(generator_loss)\n",
    "\n",
    "                # Stop training generator\n",
    "                if epoch + 1 > self.stop_epochs:\n",
    "                    stop = 1\n",
    "\n",
    "        # Detection result\n",
    "        self.decision_scores_ = self.discriminator.predict(X).ravel()\n",
    "        self._process_decision_scores()\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Predict raw anomaly score of X using the fitted detector.\n",
    "\n",
    "        The anomaly score of an input sample is computed based on different\n",
    "        detector algorithms. For consistency, outliers are assigned with\n",
    "        larger anomaly scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only\n",
    "            if they are supported by the base estimator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        anomaly_scores : numpy array of shape (n_samples,)\n",
    "            The anomaly score of the input samples.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['discriminator'])\n",
    "        X = check_array(X)\n",
    "        pred_scores = self.discriminator.predict(X).ravel()\n",
    "        return pred_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# imoprt data science libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import ML libraries\n",
    "import keras\n",
    "import model_utils as mutils\n",
    "from model_utils.evaluation import get_metrics, evaluate_model, table\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyod\n",
    "import time\n",
    "from joblib import dump, load\n",
    "\n",
    "path_to_mnt = '../data/kddcup/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_k_fold in [7]:\n",
    "  print(\"current fold: \", current_k_fold)\n",
    "\n",
    "  # set seed\n",
    "  SEED=current_k_fold**3\n",
    "  np.random.seed(SEED)\n",
    "\n",
    "\n",
    "  # deserialize pre-processed data\n",
    "  path_to_pickle = f'../data/kddcup/kdd_preprocessed_k{current_k_fold}.pkl'\n",
    "\n",
    "  with open(path_to_pickle, \"rb\") as f:\n",
    "      data = pickle.load(f)\n",
    "      X_train = data[\"X_train\"].to_numpy()\n",
    "      y_train = data[\"y_train\"].to_numpy()\n",
    "\n",
    "      X_val = data[\"X_val\"].to_numpy()\n",
    "      y_val = data[\"y_val\"].to_numpy()\n",
    "\n",
    "      X_test = data[\"X_test\"].to_numpy()\n",
    "      y_test = data[\"y_test\"].to_numpy()\n",
    "\n",
    "      col_names = data[\"col_names\"]\n",
    "\n",
    "  print(\"Data loaded successfully\")\n",
    "\n",
    "  # Reshape\n",
    "  y_train = y_train.reshape(-1, 1)\n",
    "  y_test = y_test.reshape(-1, 1)\n",
    "  y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "  # Set Weight\n",
    "  res_value_counts = df(y_train).value_counts()\n",
    "  weight_for_0 = 1.0 / res_value_counts[0]\n",
    "  weight_for_1 = 1.0 / res_value_counts[1]\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(X_train)\n",
    "\n",
    "  X_train = scaler.transform(X_train)\n",
    "  X_val = scaler.transform(X_val)\n",
    "  X_test = scaler.transform(X_test)\n",
    "\n",
    "  # run for current fold\n",
    "  contamination = len(y_train[y_train == 1]) / len(y_train) # proportion of frauds in the training dataset\n",
    "  n_sub_generators = 5\n",
    "  lr_discriminator = 0.01\n",
    "  lr_generator = 0.0001\n",
    "  epochs = 1\n",
    "\n",
    "  mo_gaal = MO_GAAL(\n",
    "      k=n_sub_generators,\n",
    "      stop_epochs=epochs,\n",
    "      contamination=contamination,\n",
    "      lr_d=lr_discriminator,\n",
    "      lr_g=lr_generator,\n",
    "  )\n",
    "\n",
    "  # train in supervised manner\n",
    "  start = time.time()\n",
    "  clf = mo_gaal.fit(X_train, y_train) # 67 min for 1 epoch and n_sub_generators = 5\n",
    "  elapsed = time.time() - start\n",
    "\n",
    "  # evaluate\n",
    "  scores = mo_gaal.predict_proba(X_test)\n",
    "  scores_normal = df(mo_gaal.predict_proba(df(X_train)[y_train == 0])[:,1])\n",
    "  scores_anomal = df(mo_gaal.predict_proba(df(X_train)[y_train == 1])[:, 0])\n",
    "\n",
    "  # let's find the best threshold\n",
    "  best_metric = 0.\n",
    "  best_th = 0.\n",
    "\n",
    "  for threshold in np.arange(0., 1.0, 0.001):\n",
    "      current_metric = get_metrics(y_test, scores[:, 1], op=\">\", threshold=threshold)[\"AUCPRC\"]\n",
    "      if current_metric > best_metric:\n",
    "          best_metric = current_metric\n",
    "          best_th = threshold\n",
    "\n",
    "  print(\"Best Metric Score:\", best_metric)\n",
    "  print(\"Best Threshold: \", best_th)\n",
    "\n",
    "  metrics = get_metrics(y_test, scores[:, 1], threshold=best_th)\n",
    "\n",
    "  print(\"metrics for fold: \", current_k_fold)\n",
    "  print(metrics)\n",
    "\n",
    "  # save model\n",
    "  dump(mo_gaal, f'./saved_models/MOGAAL/mo_gaal_KDD_k{current_k_fold}.joblib')\n",
    "\n",
    "  elapsed = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1 =  {'tn': 22674.0, 'fp': 2370.0, 'fn': 417.0, 'tp': 5642.0, 'precision': 0.7042, 'recall': 0.9312, 'AUCPRC': 0.6558, 'F1': 0.8019, 'ROCAUC': 0.941, 'MCC': 0.7576, 'ACC': 0.9104, 'GMEAN': 0.9182}\n",
    "fold2 =  {'tn': 73.0, 'fp': 24971.0, 'fn': 6.0, 'tp': 6053.0, 'precision': 0.1951, 'recall': 0.999, 'AUCPRC': 0.1554, 'F1': 0.3265, 'ROCAUC': 0.2907, 'MCC': 0.0151, 'ACC': 0.197, 'GMEAN': 0.054}\n",
    "fold3 =  {'tn': 47.0, 'fp': 24997.0, 'fn': 1.0, 'tp': 6058.0, 'precision': 0.1951, 'recall': 0.9998, 'AUCPRC': 0.1576, 'F1': 0.3265, 'ROCAUC': 0.2646, 'MCC': 0.0173, 'ACC': 0.1963, 'GMEAN': 0.0433}\n",
    "fold4 =   {'tn': 3398.0, 'fp': 21645.0, 'fn': 525.0, 'tp': 5535.0, 'precision': 0.2036, 'recall': 0.9134, 'AUCPRC': 0.1678, 'F1': 0.333, 'ROCAUC': 0.3909, 'MCC': 0.0585, 'ACC': 0.2872, 'GMEAN': 0.352}\n",
    "fold5 = {'tn': 24013.0, 'fp': 1031.0, 'fn': 3698.0, 'tp': 2361.0, 'precision': 0.696, 'recall': 0.3897, 'AUCPRC': 0.3775, 'F1': 0.4996, 'ROCAUC': 0.5483, 'MCC': 0.4428, 'ACC': 0.848, 'GMEAN': 0.6113}\n",
    "fold6 =  {'tn': 25.0, 'fp': 25018.0, 'fn': 0.0, 'tp': 6060.0, 'precision': 0.195, 'recall': 1.0, 'AUCPRC': 0.1456, 'F1': 0.3264, 'ROCAUC': 0.3249, 'MCC': 0.014, 'ACC': 0.1956, 'GMEAN': 0.0316}\n",
    "fold7 =  {'tn': 39.0, 'fp': 25005.0, 'fn': 1.0, 'tp': 6058.0, 'precision': 0.195, 'recall': 0.9998, 'AUCPRC': 0.1217, 'F1': 0.3264, 'ROCAUC': 0.2013, 'MCC': 0.0154, 'ACC': 0.196, 'GMEAN': 0.0395}\n",
    "fold8 = {'tn': 18770.0, 'fp': 6274.0, 'fn': 109.0, 'tp': 5950.0, 'precision': 0.4867, 'recall': 0.982, 'AUCPRC': 0.3842, 'F1': 0.6509, 'ROCAUC': 0.8065, 'MCC': 0.5932, 'ACC': 0.7948, 'GMEAN': 0.8579}\n",
    "fold9 = {'tn': 421.0, 'fp': 24623.0, 'fn': 76.0, 'tp': 5983.0, 'precision': 0.1955, 'recall': 0.9875, 'AUCPRC': 0.1256, 'F1': 0.3264, 'ROCAUC': 0.2353, 'MCC': 0.0135, 'ACC': 0.2059, 'GMEAN': 0.1288}\n",
    "fold10 = {'tn': 23311.0, 'fp': 1732.0, 'fn': 4261.0, 'tp': 1799.0, 'precision': 0.5095, 'recall': 0.2969, 'AUCPRC': 0.2373, 'F1': 0.3751, 'ROCAUC': 0.3163, 'MCC': 0.2843, 'ACC': 0.8073, 'GMEAN': 0.5257}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/y93p_qw91z98pk044cfd6b800000gn/T/ipykernel_49556/3826644129.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  kfold_results = pd.concat([kfold_results, df([fold])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precision    0.3576 ± 0.2186\n",
       "recall       0.8499 ± 0.2696\n",
       "AUCPRC       0.2528 ± 0.1719\n",
       "F1           0.4293 ± 0.1693\n",
       "MCC          0.2212 ± 0.2825\n",
       "GMEAN        0.3562 ± 0.3509\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "\n",
    "kfold_results = df(columns=['tn', 'fp', 'fn', 'tp', 'precision', 'recall', 'AUCPRC', 'F1', 'ROCAUC', 'MCC', 'ACC', 'GMEAN'])\n",
    "\n",
    "for fold in [fold1, fold2, fold3, fold4, fold5, fold6, fold7, fold8, fold9, fold10]:\n",
    "\tkfold_results = pd.concat([kfold_results, df([fold])], ignore_index=True)\n",
    "\n",
    "kfold_results = kfold_results.drop(['tn', 'fp', 'fn', 'tp', 'ROCAUC', 'ACC'] , axis=1)\n",
    "kfold_results.agg(lambda x: f'{x.mean():.4f} ± {x.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_results.describe().round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
