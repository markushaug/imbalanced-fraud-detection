{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Imbalance ratio in training data: 599.48\n",
      "Imbalance ratio in validation data: 590.1\n",
      "Imbalance ratio in test data: 602.68\n",
      "\n",
      "Number of samples in training data: 226980\n",
      "Number of samples in validation data: 28373\n",
      "Number of samples in test data: 28373\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# local module with helper utils\n",
    "import model_utils as mutils\n",
    "from model_utils.evaluation import get_metrics, evaluate_model\n",
    "\n",
    "current_k_fold = 10\n",
    "# set seed\n",
    "SEED=current_k_fold**3\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# pandas options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# deserialize pre-processed data\n",
    "path_to_pickle = f'../data/creditcard/cc13_preprocessed_k{current_k_fold}.pkl'\n",
    "\n",
    "with open(path_to_pickle, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "\n",
    "    X_val = data['X_val']\n",
    "    y_val = data['y_val']\n",
    "\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "print('Data loaded successfully')\n",
    "\n",
    "# get imbalance ratio for each data set\n",
    "IR_train = mutils.imb_ratio(y_train.value_counts())\n",
    "IR_val = mutils.imb_ratio(y_val.value_counts())\n",
    "IR_test = mutils.imb_ratio(y_test.value_counts())\n",
    "\n",
    "print(f\"Imbalance ratio in training data: {IR_train}\")\n",
    "print(f\"Imbalance ratio in validation data: {IR_val}\")\n",
    "print(f\"Imbalance ratio in test data: {IR_test}\")\n",
    "\n",
    "# print number of samples in each data set\n",
    "print(f\"\\nNumber of samples in training data: {len(y_train)}\")\n",
    "print(f\"Number of samples in validation data: {len(y_val)}\")\n",
    "print(f\"Number of samples in test data: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data in X_missing10:  10.0\n",
      "Missing Data in X_missing20:  20.0\n",
      "Missing Data in X_missing30:  30.0\n",
      "Missing Data in X_missing50:  50.0\n"
     ]
    }
   ],
   "source": [
    "def cutout_data(X, pct, filler=np.nan):\n",
    "\tX_missing = X.copy()\n",
    "\n",
    "\tpct_idx = np.random.choice(X_missing.index, int(len(X_missing) * pct), replace=False)\n",
    "\tpct_cols = np.random.choice(X_missing.columns, int(len(X_missing.columns) * 1), replace=False)\n",
    "\n",
    "\tX_missing.loc[pct_idx, pct_cols] = filler\n",
    "\treturn X_missing\n",
    "\n",
    "X_missing10 = cutout_data(X_train, 0.1)\n",
    "X_missing20 = cutout_data(X_train, 0.2)\n",
    "X_missing30 = cutout_data(X_train, 0.3)\n",
    "X_missing50 = cutout_data(X_train, 0.5)\n",
    "\n",
    "print(\"Missing Data in X_missing10: \", X_missing10.isna().mean().mean().round(4) * 100)\n",
    "print(\"Missing Data in X_missing20: \", X_missing20.isna().mean().mean().round(4) * 100)\n",
    "print(\"Missing Data in X_missing30: \", X_missing30.isna().mean().mean().round(4) * 100)\n",
    "print(\"Missing Data in X_missing50: \", X_missing50.isna().mean().mean().round(4) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature imputation with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# enable experimental features\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "# datasets\n",
    "datasets = {\n",
    "\t'10pct': (X_missing10, y_train),\n",
    "\t'20pct': (X_missing20, y_train),\n",
    "\t'30pct': (X_missing30, y_train),\n",
    "\t'50pct': (X_missing50, y_train)\n",
    "}\n",
    "\n",
    "\n",
    "# configure IterativeImputer\n",
    "min_values = np.concatenate([np.full(34, -1.), np.zeros(87)])\n",
    "max_values = np.concatenate([np.full(34, 1.), np.ones(87)])\n",
    "\n",
    "# define imputers\n",
    "imputers = {\n",
    "    'mean': SimpleImputer(strategy='mean'),\n",
    "    'median': SimpleImputer(strategy='median'),\n",
    "\t#'iterative': IterativeImputer(random_state=SEED, missing_values=np.nan, min_value=min_values, max_value=max_values)\n",
    "}\n",
    "\n",
    "models = []\n",
    "names = []\n",
    "\n",
    "# for each dataset\n",
    "for ds_name, ds in datasets.items():\n",
    "\t# impute with each imputer & fit on imputed data\n",
    "\tfor imp_name, imputer in imputers.items():\n",
    "\n",
    "\t\t# create pipeline\n",
    "\t\tpipeline = Pipeline(steps=[\n",
    "\t\t\t\n",
    "\t\t\t('imputer', imputer),\n",
    "\t\t\t('ros', RandomOverSampler(random_state=SEED, sampling_strategy=1)),\n",
    "\t\t\t('clf', XGBClassifier(random_state=SEED)) # vanilla XGB was the best for KDD dataset\n",
    "\t\t])\n",
    "\n",
    "\t\tx_ds, y_ds = ds\n",
    "\t\tmodels.append(pipeline.fit(x_ds, y_ds))\n",
    "\t\tnames.append(ds_name + '_' + imp_name)\n",
    "\n",
    "# evaluate\n",
    "evaluate_model(\n",
    "\tmodel=models,\n",
    "\tX=X_test,\n",
    "\ty_true=df(y_test),\n",
    "\tnames=names,\n",
    "\tas_table=True # 80m 40s\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
